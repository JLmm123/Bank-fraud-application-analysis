# -*- coding: utf-8 -*-
"""Bank analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kYarykG5T_aad6TFcrXHudnZk8-ffZSX

Lingpei(Jocelyn) Luo -- Bank Fraud Analysis for Data challenge

# Load the dateset
"""



# Sklearn and Pandas Setup
import json
import pandas as pd
import numpy as np
import datetime as dt
import re
import os
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib import cm
from google.colab import drive
from wordcloud import WordCloud

# Code to read csv file into Colaboratory:
!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
# Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

link = 'https://drive.google.com/file/d/1MEZRWv_VwQ7PxChZJu0uDNqAI9RzHldo/view?usp=share_link'
id = '1MEZRWv_VwQ7PxChZJu0uDNqAI9RzHldo'
downloaded = drive.CreateFile({'id':id}) 
downloaded.GetContentFile('sp23_datachallenge.csv')  
bank_df = pd.read_csv('sp23_datachallenge.csv')

bank_df

"""# Check rows, columns, and duplicates"""

#all rows are filled
bank_df_clean = bank_df.dropna(axis = 0, how = 'any')

def find_empty_columns(df):
    df_missing_columns = df.isnull().sum(axis=0).reset_index()
    df_missing_columns.columns = ['name', 'count']
    df_missing_columns['ratio'] = (df.shape[0]-df_missing_columns['count']) / df.shape[0]
    return df_missing_columns

# there are no columns that have no value
df_missing_columns = find_empty_columns(bank_df_clean)
print(df_missing_columns[df_missing_columns['ratio'] == 0])

#TO-DO: Drop the duplicate rows
bank_df_clean.drop_duplicates(inplace = True)

bank_df_clean

"""# EDA Analysis"""

bank_df_clean.describe()

"""From the descriptive data, we have seen that the columns of device_fraud_appl which supposed to be at range (0,1), has exclusively 0. We consider dropping this column."""

bank_df_clean.drop(columns= ['device_fraud_count'], inplace = True)

corr_matrix = bank_df_clean.corr()

corr_matrix

sns.heatmap(corr_matrix, cmap='RdBu', vmin=-1.0, vmax=1.0)
plt.title("Correlation Matrix")
plt.show()

bank_df_clean.groupby(by = "customer_age").mean()

"""From a quick EDA about customer age, we can see that, 

*   Customer with an age higher tends to have a higher tendancy of fraud. 
*   Customer with the age of 20, 30, 40 has lower current address months count. Customer with age of 20, 30 has lower value in prev address months count as well, this might indicates that people tends to relocate in their careers.
*   Customer with age of 10 has lowest fraud_bool, zip_count_4w, proposed_credit_limit. highest name_email_similarity, intended_balcon_count, session_length_in_minutes.




"""

bank_df_clean.groupby(by = "income").mean()

"""From a quick EDA about customer income, we can see that, 

*   Customer with an income (0.6, 0.7, 0.8, 0.9) higher tends to have a higher value of prev address months count and intended balcon amount. While customer with lowest income (0.1) tends to have highest value of current address months count. 
*   Customer with the income of 0.9 has a higher proposed_credit_limit and higher fraud_bool.

# Clustering
"""

from matplotlib import pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from matplotlib.colors import ListedColormap

# Commented out IPython magic to ensure Python compatibility.
from numpy.random.mtrand import normal
from sklearn import preprocessing
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans

def create_features(df):
  
    df.dropna(how = 'all', axis = 0, inplace=True)
    df.fillna(0, inplace = True)
    x = df.values # numpy array
    scaler = preprocessing.StandardScaler()
    x_scaled = scaler.fit_transform(x)

    return pd.DataFrame(x_scaled)




def run_pipeline(k, X_norm, pca_component):
    pca = PCA(n_components = 3, random_state = 0) # 3D PCA for the plot
    reduced = pd.DataFrame(pca.fit_transform(X_norm))

    kmeans = KMeans(n_clusters=k, random_state = 0)
    
    # Make this student todos 
    # fit the input data
    kmeans = kmeans.fit(reduced)
    # get the cluster labels
    labels = kmeans.predict(reduced)
    # centroid values
    centroid = kmeans.cluster_centers_
    # cluster values
    clusters = kmeans.labels_.tolist()

    reduced['cluster'] = clusters

    reduced.columns = ['x', 'y', 'z', 'cluster']

    if(pca_component):
      return reduced, pca.components_

    return reduced

def plot_curves(reduced):
#     %matplotlib inline
    sns.set(style="white")

    # axes instance
    fig = plt.figure(figsize=(6,6))
    ax = Axes3D(fig)
    fig.add_axes(ax)

    # get colormap from seaborn
    cmap = ListedColormap(sns.color_palette("husl", 256).as_hex())

    # plot
    sc = ax.scatter(xs=reduced.x, ys=reduced.y, zs=reduced.z,c=reduced.cluster, cmap=cmap, alpha=1)
    ax.set_xlabel('X Label')
    ax.set_ylabel('Y Label')
    ax.set_zlabel('Z Label')

    # legend
    plt.legend(*sc.legend_elements(), bbox_to_anchor=(1.05, 1), loc=2)


    # save
    plt.savefig("scatter_hue", bbox_inches='tight')

bank_cluster_df = bank_df_clean.drop(columns = ['payment_type', 'employment_status','housing_status', 'source','device_os'])
X_norm = create_features(bank_cluster_df)
# first, we choose our cluster number to be 15 (we will later re-choose another cluster number)
k = 15
km = run_pipeline(k, X_norm, False)
# do a groupby to see how our clusters perform
km.groupby('cluster').count().sort_values('x', ascending = False)

def deciding_k(K, X_norm):
    pca = PCA(n_components = 3, random_state = 0) # 3D PCA for the plot
    reduced = pd.DataFrame(pca.fit_transform(X_norm))
  

    #compute the distortion
    Sum_of_squared_distances = []

    for k in K:
      kmeans = KMeans(n_clusters=k, max_iter = 100, random_state = 0)  
     
      # fit the input data
      kmeans = kmeans.fit(reduced)
      # get the cluster labels
      labels = kmeans.predict(reduced)
      # centroid values
      centroid = kmeans.cluster_centers_
      # cluster values
      clusters = kmeans.labels_.tolist()
      Sum_of_squared_distances.append(kmeans.inertia_)

    return Sum_of_squared_distances

K = range(1,15)
X_norm = create_features(bank_cluster_df)
distortion = deciding_k(K, X_norm)

plt.plot(K, distortion, 'bx-')
plt.xlabel('k')
plt.ylabel('Sum_of_squared_distances')
plt.title('Elbow Method For Optimal k')
plt.show()

# 8 is our chosen cluster number 
k = 8
X_norm = create_features(bank_cluster_df)
km, pca = run_pipeline(k, X_norm, pca_component=True)

clusters_dist = km.groupby('cluster').count().sort_values('x', ascending = False)
clusters_dist.reset_index(drop = False, inplace = True)
clusters_dist['percentage'] = (clusters_dist.x/clusters_dist.x.sum())*100
sns.catplot(x= 'cluster', y = 'percentage', hue = 'cluster', data = clusters_dist, kind = 'bar', height = 6, aspect = 1.5, dodge = False )

plot_curves(km)

pca

for cluster in np.unique(km.cluster):
    display("Cluster "+str(cluster))
    display(km[km['cluster'] == cluster].describe())

"""# Gaussian Mixture Model"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from sklearn.mixture import GaussianMixture
# 
# #creatig features
# X_train = bank_cluster_df.values
# 
# #choosing cluster number
# n_clusters = 8
# 
# #create the model
# max_iter = 100
# model = GaussianMixture(n_components=n_clusters, covariance_type="full", n_init = 5, max_iter = max_iter)
# #fit model on data
# model.fit(X_train)
# 
# #predict cluster number for the date
# results = bank_cluster_df
# results["cluster"] = model.predict(X_train)



for cluster in range(0,n_clusters):
    display("Cluster "+str(cluster))
    display(results[results['cluster'] == cluster].describe())

"""# Classification Model

## Preprocessing
"""

bank_contents_df = bank_df_clean.drop(columns=['fraud_bool'])

"""## Feature Engeneering"""

bank_contents_df = pd.get_dummies(bank_contents_df, columns = ['payment_type', 'employment_status','housing_status', 'source','device_os'])
bank_contents_df

bank_contents_df.columns

"""## Prepare for modeling"""

#store the columns to be used as features in a DataFrame called "features" 
features = bank_contents_df
#store the binary classification target variable as "labels"
labels = bank_df_clean['fraud_bool']

from sklearn.model_selection import train_test_split
#conduct 80/20 train-test split
seed = 42
x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size = 0.2, random_state = seed)

"""## Imbalance"""

bank_df_clean['fraud_bool'].value_counts()

from matplotlib import pyplot as plt
bank_df_clean['fraud_bool'].value_counts().plot(kind="bar", title="test")

plt.title("Whether or not an application is fraud")
plt.xlabel("fraud")
plt.ylabel("The number of fraud")

# use over-sampling to solve data imbalance 
from imblearn.over_sampling import RandomOverSampler
rus = RandomOverSampler(random_state=42)
x_res, y_res = rus.fit_resample(x_train, y_train)

# # Create Decision Tree classifer object
# clf = DecisionTreeClassifier()

# # Train Decision Tree Classifer
# clf = clf.fit(x_res,y_res)

# #Predict the response for test dataset
# y_pred = clf.predict(x_test)

# # Model Accuracy, how often is the classifier correct?
# print("Accuracy:",metrics.accuracy_score(y_test, y_pred))

x_res.shape

"""## Logistic Regression"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Initialize model with default parameters and fit it on the training set
reg = LogisticRegression()
reg.fit(x_res, y_res)
# Use the model to predict on the test set and save these predictions as `y_pred`
y_pred = reg.predict(x_test)
# accuracy
train_acc = reg.score(x_res, y_res)
acc = accuracy_score(y_test, y_pred)

train_acc

acc

from sklearn import metrics

confusion_matrix = metrics.confusion_matrix(y_test, y_pred)

cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])

cm_display.plot()
plt.show()

y_pred_proba = reg.predict_proba(x_test)[::,1]
fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)

#create ROC curve
plt.plot(fpr,tpr)
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()

"""## Decision Tree"""

# Import the model we are using
from sklearn.ensemble import RandomForestClassifier
# Instantiate model with 10 decision trees
rf = RandomForestClassifier(n_estimators= 100, random_state=42, max_depth=10)
# Train the model on training data
rf.fit(x_res, y_res);

# Use the forest's predict method on the test data
predictions = rf.predict(x_test)

train_acc = rf.score(x_res, y_res)
test_acc = rf.score(x_test, y_test)

train_acc

test_acc

from sklearn import metrics

confusion_matrix = metrics.confusion_matrix(y_test, predictions)
    #y_test, predictions)

cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])

cm_display.plot()
plt.show()

y_pred_proba = reg.predict_proba(x_test)[::,1]
fpr, tpr, _ = metrics.roc_curve(y_test,  predictions)

#create ROC curve
plt.plot(fpr,tpr)
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()

rf.feature_importances_

def plot_feature_importance(importance,names,model_type):

  #Create arrays from feature importance and feature names
  feature_importance = np.array(importance)
  feature_names = np.array(names)

  #Create a DataFrame using a Dictionary
  data={'feature_names':feature_names,'feature_importance':feature_importance}
  fi_df = pd.DataFrame(data)

  #Sort the DataFrame in order decreasing feature importance
  fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)

  #Define size of bar plot
  plt.figure(figsize=(10,8))
  #Plot Searborn bar chart
  sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])
  #Add chart labels
  plt.title(model_type + 'FEATURE IMPORTANCE')
  plt.xlabel('FEATURE IMPORTANCE')
  plt.ylabel('FEATURE NAMES')

plot_feature_importance(rf.feature_importances_,x_res.columns,'RANDOM FOREST')